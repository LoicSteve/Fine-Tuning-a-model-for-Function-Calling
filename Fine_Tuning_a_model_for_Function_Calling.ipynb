{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPnzbcwUr9aTZBb6jCBmAhT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e6b49547993a473a9c3d2a0b2a94123b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b25c98bb3b2140dcbada48875df3a4d8",
              "IPY_MODEL_73d74033df134b27bcbaba64aadc4981",
              "IPY_MODEL_f4f945870f3c45099efce773238245a1"
            ],
            "layout": "IPY_MODEL_1e75659d413d4b25affb54e86ca56b69"
          }
        },
        "b25c98bb3b2140dcbada48875df3a4d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6ec397c784c4bec984b02f485dca6d3",
            "placeholder": "​",
            "style": "IPY_MODEL_8b80840cf1e3458ea79682d21a5bec49",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "73d74033df134b27bcbaba64aadc4981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f12ae1312dff4545b7430ce3e9e7cdc0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4d18ad537344c21a2e815bc0f5c8316",
            "value": 2
          }
        },
        "f4f945870f3c45099efce773238245a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aef24e0d7cae47b5a10fe76290f3f99e",
            "placeholder": "​",
            "style": "IPY_MODEL_1976aabbc33240f2b4d553ce534ac9d0",
            "value": " 2/2 [00:12&lt;00:00,  5.31s/it]"
          }
        },
        "1e75659d413d4b25affb54e86ca56b69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6ec397c784c4bec984b02f485dca6d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b80840cf1e3458ea79682d21a5bec49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f12ae1312dff4545b7430ce3e9e7cdc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4d18ad537344c21a2e815bc0f5c8316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aef24e0d7cae47b5a10fe76290f3f99e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1976aabbc33240f2b4d553ce534ac9d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16ef8ee42bc3412493e899d4e9be9d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_314bab8e1ef445468bb231cc72b7c4ac",
              "IPY_MODEL_51421d382fbb46f7a9080ff4e1a940a1",
              "IPY_MODEL_5277f997002642dabaebe57382efd51e"
            ],
            "layout": "IPY_MODEL_5e51b79c7ce443bba141eddab7f12a2d"
          }
        },
        "314bab8e1ef445468bb231cc72b7c4ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c100bf44794457699066dab6d9b2801",
            "placeholder": "​",
            "style": "IPY_MODEL_9c05bc4ce62842a087002d1ef1d787fe",
            "value": "Tokenizing train dataset: 100%"
          }
        },
        "51421d382fbb46f7a9080ff4e1a940a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bc643540d8f4eada3170cb88ea8d176",
            "max": 3213,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_acc55c3b97aa4b7cb3b07d03234d3233",
            "value": 3213
          }
        },
        "5277f997002642dabaebe57382efd51e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddbc83374d32436192432a48c76157f6",
            "placeholder": "​",
            "style": "IPY_MODEL_d574f7bc224a46e495ad45f1dd1794e7",
            "value": " 3213/3213 [00:07&lt;00:00, 349.57 examples/s]"
          }
        },
        "5e51b79c7ce443bba141eddab7f12a2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c100bf44794457699066dab6d9b2801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c05bc4ce62842a087002d1ef1d787fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bc643540d8f4eada3170cb88ea8d176": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acc55c3b97aa4b7cb3b07d03234d3233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ddbc83374d32436192432a48c76157f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d574f7bc224a46e495ad45f1dd1794e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5170d25abe834ce8a092a4d513d4a605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2f33aaf321841d1939f7155b7cd5536",
              "IPY_MODEL_71c496af487f460ebb705ebda972f088",
              "IPY_MODEL_d6510e3b5355493699bf2c8ef40174dc"
            ],
            "layout": "IPY_MODEL_a841ff3d037d4e19962dcb89e23c63c4"
          }
        },
        "b2f33aaf321841d1939f7155b7cd5536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_283a377e560c4374a2ed8aaf6410cf81",
            "placeholder": "​",
            "style": "IPY_MODEL_a5f69f521afa4f29bb66d8de2e679bf2",
            "value": "Packing train dataset: 100%"
          }
        },
        "71c496af487f460ebb705ebda972f088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c637e5043cf4b799b1aa1916908ec58",
            "max": 3213,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d7dca31cdfb4a079b4ce3b1d4cabd54",
            "value": 3213
          }
        },
        "d6510e3b5355493699bf2c8ef40174dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5e0088197504671889bbf1dd9c84072",
            "placeholder": "​",
            "style": "IPY_MODEL_6970c3b30b974783b585866c5db5b84f",
            "value": " 3213/3213 [00:09&lt;00:00, 365.89 examples/s]"
          }
        },
        "a841ff3d037d4e19962dcb89e23c63c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "283a377e560c4374a2ed8aaf6410cf81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5f69f521afa4f29bb66d8de2e679bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c637e5043cf4b799b1aa1916908ec58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d7dca31cdfb4a079b4ce3b1d4cabd54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5e0088197504671889bbf1dd9c84072": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6970c3b30b974783b585866c5db5b84f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoicSteve/Fine-Tuning-a-model-for-Function-Calling/blob/main/Fine_Tuning_a_model_for_Function_Calling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning a model for Function-Calling\n",
        "\n",
        "**we're going to Fine-Tune an LLM for Function Calling.**\n",
        "\n",
        "This notebook is part of the <a href=\"https://www.hf.co/learn/agents-course/unit1/introduction\">Hugging Face Agents Course</a>, a free Course from beginner to expert, where you learn to build Agents."
      ],
      "metadata": {
        "id": "zK4Gc4wFybX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Ask to Access Gemma on Hugging Face\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit1/gemma.png\" alt=\"Gemma\"/>\n",
        "\n",
        "\n",
        "To access Gemma on Hugging Face:\n",
        "\n",
        "1. **Make sure you're signed in** to your Hugging Face Account\n",
        "\n",
        "2. Go to https://huggingface.co/google/gemma-2-2b-it\n",
        "\n",
        "3. Click on **Acknowledge license** and fill the form.\n",
        "\n",
        "Alternatively you can use another model, and modify the code accordingly (it can be a good exercise for you to be sure you know how to fine-tune for Function-Calling).\n",
        "\n",
        "You can use for instance:\n",
        "\n",
        "- [HuggingFaceTB/SmolLM2-1.7B-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct)\n",
        "\n",
        "- [meta-llama/Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)"
      ],
      "metadata": {
        "id": "jer9yLaRyyI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Set the GPU 💪\n",
        "\n",
        "If you're on Colab:\n",
        "\n",
        "- To **accelerate the fine-tuning training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\"/>\n",
        "\n",
        "- `Hardware Accelerator > GPU`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\"/>\n",
        "\n",
        "\n",
        "### Important\n",
        "\n",
        "For this Unit, **with the free-tier of Colab** it will take around **6h to train**.\n",
        "\n",
        "You have three solutions if you want to make it faster:\n",
        "\n",
        "1. Train on your computer if you have GPUs. It might take time but you have less risks of timeout.\n",
        "\n",
        "2. Use a Google Colab Pro that allows you use to A100 GPU (15-20min training).\n",
        "\n",
        "3. Just follow the code to learn how to do it without training."
      ],
      "metadata": {
        "id": "5jm2sGrvzV_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Install dependencies 📚\n",
        "\n",
        "We need multiple librairies:\n",
        "\n",
        "- `bitsandbytes` for quantization\n",
        "- `peft`for LoRA adapters\n",
        "- `Transformers`for loading the model\n",
        "- `datasets`for loading and using the fine-tuning dataset\n",
        "- `trl`for the trainer class"
      ],
      "metadata": {
        "id": "dV-6mnsuzhVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U trl\n",
        "!pip install -q -U tensorboardX\n",
        "!pip install -q wandb"
      ],
      "metadata": {
        "id": "06CuDnokyuqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Create your Hugging Face Token to push your model to the Hub\n",
        "\n",
        "To be able to share your model with the community there are some more steps to follow:\n",
        "\n",
        "1️⃣ (If it's not already done) create an account to HF ➡ https://huggingface.co/join\n",
        "\n",
        "2️⃣ Sign in and then, you need to store your authentication token from the Hugging Face website.\n",
        "\n",
        "- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit1/create_write_token.png\" alt=\"Create HF Token\" width=\"50%\">\n",
        "\n",
        "3️⃣ Store your token as an environment variable under the name \"HF_TOKEN\"\n",
        "- **Be very carefull not to share it with others** !"
      ],
      "metadata": {
        "id": "nZZ3h19Zzr1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Import the librairies\n",
        "\n",
        "Don't forget to put your HF token."
      ],
      "metadata": {
        "id": "K-8sNYLaz__Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from functools import partial\n",
        "import pandas as pd\n",
        "import torch\n",
        "import json\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
        "from datasets import load_dataset\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from peft import LoraConfig, TaskType\n",
        "\n",
        "seed = 42\n",
        "set_seed(seed)\n",
        "\n",
        "import os\n",
        "\n",
        "# Put your HF Token here\n",
        "os.environ['HF_TOKEN']=\"hf-XXX\" # the token should have write access"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkZMxuXNzkaC",
        "outputId": "64bfda21-afcc-44c8-cbd5-0c4498a04452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Processing the dataset into inputs\n",
        "\n",
        "In order to train the model, we need to **format the inputs into what we want the model to learn**.\n",
        "\n",
        "For this tutorial, I enhanced a popular dataset for function calling \"NousResearch/hermes-function-calling-v1\" by adding some new **thinking** step computer from **deepseek-ai/DeepSeek-R1-Distill-Qwen-32B**.\n",
        "\n",
        "But in order for the model to learn, we need **to format the conversation correctly**. If you followed Unit 1, you know that going from a list of messages to a prompt is handled by the **chat_template**, or, the default chat_template of gemma-2-2B does not contain tool calls. So we will need to modify it !\n",
        "\n",
        "This is the role of our **preprocess** function. To go from a list of messages, to a prompt that the model can understand.\n"
      ],
      "metadata": {
        "id": "lopcVdyy1py2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/gemma-2-2b-it\"\n",
        "dataset_name = \"Jofthomas/hermes-function-calling-thinking-V1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
        "\n",
        "\n",
        "def preprocess(sample):\n",
        "      messages = sample[\"messages\"]\n",
        "      first_message = messages[0]\n",
        "\n",
        "      # Instead of adding a system message, we merge the content into the first user message\n",
        "      if first_message[\"role\"] == \"system\":\n",
        "          system_message_content = first_message[\"content\"]\n",
        "          # Merge system content with the first user message\n",
        "          messages[1][\"content\"] = system_message_content + \"Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\n\" + messages[1][\"content\"]\n",
        "          # Remove the system message from the conversation\n",
        "          messages.pop(0)\n",
        "\n",
        "      return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
        "\n",
        "\n",
        "\n",
        "dataset = load_dataset(dataset_name)\n",
        "dataset = dataset.rename_column(\"conversations\", \"messages\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKwa6-av1gQ5",
        "outputId": "2b8e4357-b6e0-4823-870f-c0126d66dbb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: A Dedicated Dataset for This Unit\n",
        "\n",
        "For this Bonus Unit, we created a custom dataset based on [NousResearch/hermes-function-calling-v1](https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1), which is considered a **reference** when it comes to function-calling datasets.\n",
        "\n",
        "While the original dataset is excellent, it does **not** include a **“thinking”** step.\n",
        "\n",
        "In Function-Calling, such a step is optional, but recent work—like the **deepseek** model or the paper [\"Test-Time Compute\"](https://huggingface.co/papers/2408.03314)—suggests that giving an LLM time to “think” before it answers (or in this case, **before** taking an action) can **significantly improve** model performance.\n",
        "\n",
        "I, decided to then compute a subset of this dataset and to give it to [deepseek-ai/DeepSeek-R1-Distill-Qwen-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) in order to compute some thinking tokens `<think>` before any function call. Which resulted in the following dataset :\n",
        "![Input Dataset](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit1/dataset_function_call.png)\n"
      ],
      "metadata": {
        "id": "XtRbmaJ52y4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(preprocess, remove_columns=\"messages\")\n",
        "dataset = dataset[\"train\"].train_test_split(0.1)\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TYODVG02PlQ",
        "outputId": "8c52f26c-aff3-4c1e-b887-87a92f583cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3213\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 357\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Checking the inputs\n",
        "\n",
        "Let's manually look at what an input looks like !\n",
        "\n",
        "In this example we have :\n",
        "\n",
        "1. A *User message* containing the **necessary information with the list of available tools** inbetween `<tools></tools>` then the user query, here:  `\"Can you get me the latest news headlines for the United States?\"`\n",
        "\n",
        "2. An *Assistant message* here called \"model\" to fit the criterias from gemma models containing two new phases, a **\"thinking\"** phase contained in `<think></think>` and an **\"Act\"** phase contained in `<tool_call></<tool_call>`.\n",
        "\n",
        "3. If the model contains a `<tools_call>`, we will append the result of this action in a new **\"Tool\"** message containing a `<tool_response></tool_response>` with the answer from the tool."
      ],
      "metadata": {
        "id": "vmtAIgty4B8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at how we formatted the dataset\n",
        "print(dataset[\"train\"][8][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DUkNbTT22-I",
        "outputId": "1ddcf065-2e1b-4f17-9004-3f7d9beef0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>human\n",
            "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'get_news_headlines', 'description': 'Get the latest news headlines', 'parameters': {'type': 'object', 'properties': {'country': {'type': 'string', 'description': 'The country for which headlines are needed'}}, 'required': ['country']}}}, {'type': 'function', 'function': {'name': 'search_recipes', 'description': 'Search for recipes based on ingredients', 'parameters': {'type': 'object', 'properties': {'ingredients': {'type': 'array', 'items': {'type': 'string'}, 'description': 'The list of ingredients'}}, 'required': ['ingredients']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
            "<tool_call>\n",
            "{tool_call}\n",
            "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
            "\n",
            "Can you get me the latest news headlines for the United States?<end_of_turn><eos>\n",
            "<start_of_turn>model\n",
            "<think>Okay, so the user asked for the latest news headlines in the United States. I need to figure out how to respond appropriately using the tools available.\n",
            "\n",
            "Looking at the tools provided, there's a function called get_news_headlines. That seems perfect for this situation. It requires a country code, which the user specified as the United States.\n",
            "\n",
            "Since the user is asking specifically about the US, I'll use the country parameter as 'United States' in the function call. This should fetch the relevant news for them.\n",
            "\n",
            "I should make sure to format the response correctly as a JSON object with the function name and arguments.\n",
            "</think><tool_call>\n",
            "{'name': 'get_news_headlines', 'arguments': {'country': 'United States'}}\n",
            "</tool_call><end_of_turn><eos>\n",
            "<start_of_turn>tool\n",
            "<tool_response>\n",
            "{'headlines': ['US President signs new bill into law', 'Major earthquake hits California', 'NASA announces new Mars mission', 'Tech giant releases new smartphone', 'US economy shows signs of recovery']}\n",
            "</tool_response><end_of_turn><eos>\n",
            "<start_of_turn>model\n",
            "Here are the latest news headlines for the United States:\n",
            "1. US President signs new bill into law\n",
            "2. Major earthquake hits California\n",
            "3. NASA announces new Mars mission\n",
            "4. Tech giant releases new smartphone\n",
            "5. US economy shows signs of recovery<end_of_turn><eos>\n",
            "<start_of_turn>human\n",
            "That's interesting. Can you get me the news for France now?<end_of_turn><eos>\n",
            "<start_of_turn>model\n",
            "<think>Okay, the user initially asked for the latest US news, so I used get_news_headlines with 'United States'. Now the user is asking for France, so I'll call the same function with 'France' as the country. That makes sense because the function is designed for any country.\n",
            "</think><tool_call>\n",
            "{'name': 'get_news_headlines', 'arguments': {'country': 'France'}}\n",
            "</tool_call><end_of_turn><eos>\n",
            "<start_of_turn>tool\n",
            "<tool_response>\n",
            "{'headlines': ['French President announces new environmental policy', 'Paris Fashion Week highlights', 'France wins World Cup qualifier', 'New culinary trend sweeps across France', 'French tech startup raises millions in funding']}\n",
            "</tool_response><end_of_turn><eos>\n",
            "<start_of_turn>model\n",
            "Here are the latest news headlines for France:\n",
            "1. French President announces new environmental policy\n",
            "2. Paris Fashion Week highlights\n",
            "3. France wins World Cup qualifier\n",
            "4. New culinary trend sweeps across France\n",
            "5. French tech startup raises millions in funding<end_of_turn><eos>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "print(tokenizer.pad_token)\n",
        "print(tokenizer.eos_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8s06rYT4FrR",
        "outputId": "a57a2d39-3f0a-4c9c-9770-ac095b81a2b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad>\n",
            "<eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Let's Modify the Tokenizer\n",
        "\n",
        "Indeed, as we saw in Unit 1, the tokenizer splits text into sub-words by default. This is **not** what we want for our new special tokens!\n",
        "\n",
        "While we segmented our example using `<think>`, `<tool_call>`, and `<tool_response>`, the tokenizer does **not** yet treat them as whole tokens—it still tries to break them down into smaller pieces. To ensure the model correctly interprets our new format, we must **add these tokens** to our tokenizer.\n",
        "\n",
        "Additionally, since we changed the `chat_template` in our **preprocess** function to format conversations as messages within a prompt, we also need to modify the `chat_template` in the tokenizer to reflect these changes."
      ],
      "metadata": {
        "id": "feO_yisO6GZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatmlSpecialTokens(str, Enum):\n",
        "    tools = \"<tools>\"\n",
        "    eotools = \"</tools>\"\n",
        "    think = \"<think>\"\n",
        "    eothink = \"</think>\"\n",
        "    tool_call=\"<tool_call>\"\n",
        "    eotool_call=\"</tool_call>\"\n",
        "    tool_response=\"<tool_reponse>\"\n",
        "    eotool_response=\"</tool_reponse>\"\n",
        "    pad_token = \"<pad>\"\n",
        "    eos_token = \"<eos>\"\n",
        "    @classmethod\n",
        "    def list(cls):\n",
        "        return [c.value for c in cls]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        pad_token=ChatmlSpecialTokens.pad_token.value,\n",
        "        additional_special_tokens=ChatmlSpecialTokens.list()\n",
        "    )\n",
        "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             attn_implementation='eager',\n",
        "                                             device_map=\"auto\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(torch.bfloat16)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587,
          "referenced_widgets": [
            "e6b49547993a473a9c3d2a0b2a94123b",
            "b25c98bb3b2140dcbada48875df3a4d8",
            "73d74033df134b27bcbaba64aadc4981",
            "f4f945870f3c45099efce773238245a1",
            "1e75659d413d4b25affb54e86ca56b69",
            "d6ec397c784c4bec984b02f485dca6d3",
            "8b80840cf1e3458ea79682d21a5bec49",
            "f12ae1312dff4545b7430ce3e9e7cdc0",
            "b4d18ad537344c21a2e815bc0f5c8316",
            "aef24e0d7cae47b5a10fe76290f3f99e",
            "1976aabbc33240f2b4d553ce534ac9d0"
          ]
        },
        "id": "MgRtn5Nx4WbN",
        "outputId": "869bc2a4-bf93-4792-84be-bcc30bcec584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6b49547993a473a9c3d2a0b2a94123b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Gemma2ForCausalLM(\n",
              "  (model): Gemma2Model(\n",
              "    (embed_tokens): Embedding(256008, 2304, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-25): 26 x Gemma2DecoderLayer(\n",
              "        (self_attn): Gemma2Attention(\n",
              "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
              "        )\n",
              "        (mlp): Gemma2MLP(\n",
              "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
              "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
              "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
              "          (act_fn): PytorchGELUTanh()\n",
              "        )\n",
              "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "    (rotary_emb): Gemma2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2304, out_features=256008, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Let's configure the LoRA\n",
        "\n",
        "This is we are going to define the parameter of our adapter. Those a the most important parameters in LoRA as they define the size and importance of the adapters we are training."
      ],
      "metadata": {
        "id": "qyRfDCgs6Qhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "# TODO: Configure LoRA parameters\n",
        "# r: rank dimension for LoRA update matrices (smaller = more compression)\n",
        "rank_dimension = 16\n",
        "# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\n",
        "lora_alpha = 64\n",
        "# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\n",
        "lora_dropout = 0.05\n",
        "\n",
        "peft_config = LoraConfig(r=rank_dimension,\n",
        "                         lora_alpha=lora_alpha,\n",
        "                         lora_dropout=lora_dropout,\n",
        "                         target_modules=[\"gate_proj\",\"q_proj\",\"lm_head\",\"o_proj\",\"k_proj\",\"embed_tokens\",\"down_proj\",\"up_proj\",\"v_proj\"], # wich layer in the transformers do we target ?\n",
        "                         task_type=TaskType.CAUSAL_LM)"
      ],
      "metadata": {
        "id": "75Mzj-8B6I9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Let's define the Trainer and the Fine-Tuning hyperparameters\n",
        "\n",
        "In this step, we define the Trainer, the class that we use to fine-tune our model and the hyperparameters."
      ],
      "metadata": {
        "id": "55jCKUnd6aOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "username=\"LoicSteve\"# REPLCAE with your Hugging Face username\n",
        "output_dir = \"gemma-2-2B-it-thinking-function_calling-V0\" # The directory where the trained model checkpoints, logs, and other artifacts will be saved. It will also be the default name of the model when pushed to the hub if not redefined later.\n",
        "per_device_train_batch_size = 1\n",
        "per_device_eval_batch_size = 1\n",
        "gradient_accumulation_steps = 4\n",
        "logging_steps = 5\n",
        "learning_rate = 1e-4 # The initial learning rate for the optimizer.\n",
        "\n",
        "max_grad_norm = 1.0\n",
        "num_train_epochs=1\n",
        "warmup_ratio = 0.1\n",
        "lr_scheduler_type = \"cosine\"\n",
        "max_seq_length = 1500\n",
        "\n",
        "training_arguments = SFTConfig(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    save_strategy=\"no\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\",\n",
        "    bf16=True,\n",
        "    hub_private_repo=False,\n",
        "    push_to_hub=False,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    packing=True,\n",
        "    max_seq_length=max_seq_length,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CUkTFR76azr",
        "outputId": "b453ede5-74d0-44d2-e550-af9adafd2fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As Trainer, we use the `SFTTrainer` which is a Supervised Fine-Tuning Trainer."
      ],
      "metadata": {
        "id": "8XPTaobD7yut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=peft_config,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405,
          "referenced_widgets": [
            "16ef8ee42bc3412493e899d4e9be9d7b",
            "314bab8e1ef445468bb231cc72b7c4ac",
            "51421d382fbb46f7a9080ff4e1a940a1",
            "5277f997002642dabaebe57382efd51e",
            "5e51b79c7ce443bba141eddab7f12a2d",
            "8c100bf44794457699066dab6d9b2801",
            "9c05bc4ce62842a087002d1ef1d787fe",
            "3bc643540d8f4eada3170cb88ea8d176",
            "acc55c3b97aa4b7cb3b07d03234d3233",
            "ddbc83374d32436192432a48c76157f6",
            "d574f7bc224a46e495ad45f1dd1794e7",
            "5170d25abe834ce8a092a4d513d4a605",
            "b2f33aaf321841d1939f7155b7cd5536",
            "71c496af487f460ebb705ebda972f088",
            "d6510e3b5355493699bf2c8ef40174dc",
            "a841ff3d037d4e19962dcb89e23c63c4",
            "283a377e560c4374a2ed8aaf6410cf81",
            "a5f69f521afa4f29bb66d8de2e679bf2",
            "0c637e5043cf4b799b1aa1916908ec58",
            "0d7dca31cdfb4a079b4ce3b1d4cabd54",
            "d5e0088197504671889bbf1dd9c84072",
            "6970c3b30b974783b585866c5db5b84f"
          ]
        },
        "id": "bJby71MY7xbR",
        "outputId": "3d5465ac-269f-468d-95e0-7c48452b2d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:bitsandbytes.cextension:Could not load bitsandbytes native library: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py\", line 85, in <module>\n",
            "    lib = get_native_library()\n",
            "          ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py\", line 72, in get_native_library\n",
            "    dll = ct.cdll.LoadLibrary(str(binary_path))\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 454, in LoadLibrary\n",
            "    return self._dlltype(name)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "OSError: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so)\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:543: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/3213 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16ef8ee42bc3412493e899d4e9be9d7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Packing train dataset:   0%|          | 0/3213 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5170d25abe834ce8a092a4d513d4a605"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we launch the training 🔥. Perfect time for you to pause and grab a coffee ☕."
      ],
      "metadata": {
        "id": "U_yc2LyZ760l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "D2qsc-pH77Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Let's push the Model and the Tokenizer to the Hub\n",
        "\n",
        "Let's push our model and out tokenizer to the Hub ! The model will be pushed under your username + the output_dir that we specified earlier."
      ],
      "metadata": {
        "id": "w3_clZrQ7_LS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(f\"{username}/{output_dir}\")"
      ],
      "metadata": {
        "id": "iKWeUAEe8Dd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we also modified the chat_template Which is contained in the tokenizer, let's also push the tokenizer with the model."
      ],
      "metadata": {
        "id": "0rz1wHfpEGBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.eos_token = \"<eos>\"\n",
        "# # push the tokenizer to hub ( replace with your username and your previously specified\n",
        "# tokenizer.push_to_hub(f\"{username}/{output_dir}\", token=True)"
      ],
      "metadata": {
        "id": "UL-AXEW_EHlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12: Let's now test our model !\n",
        "\n",
        "To so, we will :\n",
        "\n",
        "1. Load the adapter from the hub !\n",
        "2. Load the base model : **\"google/gemma-2-2b-it\"** from the hub\n",
        "3. Resize the model to with the new tokens we introduced !"
      ],
      "metadata": {
        "id": "JRHUFXY8EN6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "\n",
        "peft_model_id = f\"{username}/{output_dir}\" # replace with your newly trained adapter\n",
        "device = \"auto\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             )\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "model.to(torch.bfloat16)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "vhgSPpVpEOY1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}